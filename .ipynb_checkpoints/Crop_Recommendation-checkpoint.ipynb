{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "6f935018-9886-4a79-9737-f65da7f0ae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356d8fa5-4228-4c5e-82ff-0e7d99a668e5",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "5737ae47-054e-4b90-b00c-bf9c32f1e34e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (2165519512.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[184], line 6\u001b[1;36m\u001b[0m\n\u001b[1;33m    def fit(self, X, y)\u001b[0m\n\u001b[1;37m                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "class CustomDecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "    # Build the decision tree\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y, depth=0)\n",
    "        \n",
    "    # Predict class for each sample in X\n",
    "    def predict(self, X):\n",
    "        return np.array([self._classify(x, self.tree) for x in X])\n",
    "\n",
    "    # Stop conditions: max depth reached or no samples left\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        if len(y) == 0 or depth == self.max_depth:\n",
    "            # If no samples or max depth reached, return the most frequent class\n",
    "            return np.argmax(np.bincount(y)) if len(y) > 0 else None\n",
    "\n",
    "        # Find the best feature and threshold for splitting\n",
    "        best_feature, best_threshold = self._best_split(X, y)\n",
    "        if best_feature is None:\n",
    "            # If no valid split, return the most frequent class\n",
    "            return np.argmax(np.bincount(y))\n",
    "\n",
    "        # Split data into left and right subsets\n",
    "        left_idx = X[:, best_feature] < best_threshold\n",
    "        right_idx = ~left_idx\n",
    "\n",
    "        # Handle empty left or right split\n",
    "        if np.sum(left_idx) == 0 or np.sum(right_idx) == 0:\n",
    "            return np.argmax(np.bincount(y))  # Return the most frequent class\n",
    "\n",
    "        left_subtree = self._build_tree(X[left_idx], y[left_idx], depth + 1)\n",
    "        right_subtree = self._build_tree(X[right_idx], y[right_idx], depth + 1)\n",
    "\n",
    "        # Return the node as a dictionary\n",
    "        return {'feature': best_feature,\n",
    "                'threshold': best_threshold,\n",
    "                'left': left_subtree,\n",
    "                'right': right_subtree}\n",
    "\n",
    "    # Iterate through features and thresholds to find the best split\n",
    "    def _best_split(self, X, y):\n",
    "        best_gain = -1\n",
    "        best_feature, best_threshold = None, None\n",
    "        for feature in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                gain = self._information_gain(X[:, feature], y, threshold)\n",
    "                if gain > best_gain:\n",
    "                    best_gain, best_feature, best_threshold = gain, feature, threshold\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    # Calculate entropy-based information gain for a split\n",
    "    def _information_gain(self, X_feature, y, threshold):\n",
    "        parent_entropy = self._entropy(y)\n",
    "        left_idx = X_feature < threshold\n",
    "        right_idx = ~left_idx\n",
    "        if len(y[left_idx]) == 0 or len(y[right_idx]) == 0:\n",
    "            return 0\n",
    "        n = len(y)\n",
    "        n_left, n_right = len(y[left_idx]), len(y[right_idx])\n",
    "        e_left, e_right = self._entropy(y[left_idx]), self._entropy(y[right_idx])\n",
    "        child_entropy = (n_left / n) * e_left + (n_right / n) * e_right\n",
    "        return parent_entropy - child_entropy\n",
    "\n",
    "    # Calculate entropy of a label distribution\n",
    "    def _entropy(self, y):\n",
    "        proportions = np.bincount(y) / len(y)\n",
    "        return -np.sum([p * np.log2(p) for p in proportions if p > 0])\n",
    "\n",
    "    # Traverse the tree to classify a single sample\n",
    "    def _classify(self, x, tree):\n",
    "        if not isinstance(tree, dict):\n",
    "            return tree\n",
    "        feature, threshold = tree['feature'], tree['threshold']\n",
    "        if x[feature] < threshold:\n",
    "            return self._classify(x, tree['left'])\n",
    "        return self._classify(x, tree['right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "65f6f8e7-dec7-42ea-a6c9-bbe715d762f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define Custom Random Forest Classifier\n",
    "class CustomRandomForest:\n",
    "    def __init__(self, n_estimators=100, max_depth=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Train multiple decision trees on random subsets of data\n",
    "        for _ in range(self.n_estimators):\n",
    "            idxs = np.random.choice(len(X), size=len(X), replace=True)\n",
    "            X_sample, y_sample = X[idxs], y[idxs]\n",
    "            tree = CustomDecisionTree(max_depth=self.max_depth)\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Aggregate predictions from all trees\n",
    "        tree_predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        return np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=tree_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c93ba1a3-0c77-49b2-b76f-0e9b9d82f631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define Custom Naive Bayes Classifier\n",
    "class CustomNaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.class_probs = {}  # Class probabilities\n",
    "        self.feature_probs = {}  # Feature probabilities for each class\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Calculate class probabilities P(y)\n",
    "        self.class_probs = {cls: np.mean(y == cls) for cls in np.unique(y)}\n",
    "        \n",
    "        # Calculate feature probabilities P(x|y) for each class\n",
    "        self.feature_probs = {}\n",
    "        for cls in np.unique(y):\n",
    "            X_class = X[y == cls]\n",
    "            feature_probs_class = []\n",
    "            for feature_idx in range(X.shape[1]):\n",
    "                # Calculate mean and std deviation for each feature in each class\n",
    "                mean = np.mean(X_class[:, feature_idx])\n",
    "                std = np.std(X_class[:, feature_idx])\n",
    "                feature_probs_class.append((mean, std))\n",
    "            self.feature_probs[cls] = feature_probs_class\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predict the class label for each sample in X\n",
    "        return np.array([self._predict_sample(x) for x in X])\n",
    "\n",
    "    def _predict_sample(self, x):\n",
    "        # Calculate the log-probability for each class\n",
    "        class_scores = {}\n",
    "        for cls in self.class_probs:\n",
    "            class_prob = np.log(self.class_probs[cls])  # P(y)\n",
    "            feature_probs = self.feature_probs[cls]\n",
    "            # Add the log-likelihood for each feature P(x_i | y)\n",
    "            for feature_idx, (mean, std) in enumerate(feature_probs):\n",
    "                feature_prob = self._gaussian_pdf(x[feature_idx], mean, std)\n",
    "                class_prob += np.log(feature_prob)\n",
    "            class_scores[cls] = class_prob\n",
    "        return max(class_scores, key=class_scores.get)\n",
    "\n",
    "    def _gaussian_pdf(self, x, mean, std):\n",
    "        # Gaussian Probability Density Function\n",
    "        return (1 / (std * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mean) ** 2 / std ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "584d10c3-f993-483b-83cd-aca2fcd4a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Load and Prepare Data\n",
    "df = pd.read_csv(\"C:/Users/murar/Downloads/Crop_recommendation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "50586258-1cb0-4911-832f-57f906c8f673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target labels (y)\n",
    "X = df.drop(columns=[\"label\"]).to_numpy()\n",
    "y = df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e98c7ecb-b6d3-4236-865f-9f452ee69a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode string labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "2823230f-430d-400c-8a4d-09d221c2917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "12b28468-115b-4b84-91ef-a897a5906fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        23\n",
      "           1       1.00      1.00      1.00        21\n",
      "           2       1.00      1.00      1.00        20\n",
      "           3       1.00      0.96      0.98        26\n",
      "           4       1.00      0.96      0.98        27\n",
      "           5       1.00      1.00      1.00        17\n",
      "           6       1.00      1.00      1.00        17\n",
      "           7       1.00      1.00      1.00        14\n",
      "           8       0.85      0.96      0.90        23\n",
      "           9       1.00      0.95      0.97        20\n",
      "          10       0.92      1.00      0.96        11\n",
      "          11       1.00      1.00      1.00        21\n",
      "          12       1.00      1.00      1.00        19\n",
      "          13       0.96      0.96      0.96        24\n",
      "          14       1.00      1.00      1.00        19\n",
      "          15       1.00      1.00      1.00        17\n",
      "          16       1.00      1.00      1.00        14\n",
      "          17       0.96      1.00      0.98        23\n",
      "          18       1.00      1.00      1.00        23\n",
      "          19       0.96      1.00      0.98        23\n",
      "          20       0.94      0.79      0.86        19\n",
      "          21       1.00      1.00      1.00        19\n",
      "\n",
      "    accuracy                           0.98       440\n",
      "   macro avg       0.98      0.98      0.98       440\n",
      "weighted avg       0.98      0.98      0.98       440\n",
      "\n",
      "Decision Tree Accuracy: 0.9795454545454545\n"
     ]
    }
   ],
   "source": [
    "# 5. Train and Evaluate Decision Tree Model\n",
    "dt_model = CustomDecisionTree(max_depth=10)\n",
    "dt_model.fit(xtrain, ytrain)\n",
    "dt_predictions = dt_model.predict(xtest)\n",
    "print(\"Decision Tree Classification Report:\")\n",
    "print(classification_report(ytest, dt_predictions))\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(ytest, dt_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ec3b0816-e3ed-47b8-b9ec-f3c8caf59b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        23\n",
      "           1       1.00      1.00      1.00        21\n",
      "           2       1.00      0.90      0.95        20\n",
      "           3       1.00      1.00      1.00        26\n",
      "           4       1.00      1.00      1.00        27\n",
      "           5       0.94      1.00      0.97        17\n",
      "           6       1.00      1.00      1.00        17\n",
      "           7       1.00      1.00      1.00        14\n",
      "           8       0.90      0.83      0.86        23\n",
      "           9       1.00      0.95      0.97        20\n",
      "          10       0.92      1.00      0.96        11\n",
      "          11       1.00      1.00      1.00        21\n",
      "          12       1.00      1.00      1.00        19\n",
      "          13       0.88      0.96      0.92        24\n",
      "          14       1.00      1.00      1.00        19\n",
      "          15       1.00      1.00      1.00        17\n",
      "          16       1.00      1.00      1.00        14\n",
      "          17       1.00      1.00      1.00        23\n",
      "          18       1.00      1.00      1.00        23\n",
      "          19       1.00      1.00      1.00        23\n",
      "          20       0.85      0.89      0.87        19\n",
      "          21       1.00      1.00      1.00        19\n",
      "\n",
      "    accuracy                           0.98       440\n",
      "   macro avg       0.98      0.98      0.98       440\n",
      "weighted avg       0.98      0.98      0.98       440\n",
      "\n",
      "Random Forest Accuracy: 0.9772727272727273\n"
     ]
    }
   ],
   "source": [
    "# 6. Train and Evaluate Random Forest Model\n",
    "rf_model = CustomRandomForest(n_estimators=10, max_depth=10)\n",
    "rf_model.fit(xtrain, ytrain)\n",
    "rf_predictions = rf_model.predict(xtest)\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(ytest, rf_predictions))\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(ytest, rf_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "4a5a8257-d658-4865-a5c1-5e27459cae44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        23\n",
      "           1       1.00      1.00      1.00        21\n",
      "           2       1.00      1.00      1.00        20\n",
      "           3       1.00      1.00      1.00        26\n",
      "           4       1.00      1.00      1.00        27\n",
      "           5       1.00      1.00      1.00        17\n",
      "           6       1.00      1.00      1.00        17\n",
      "           7       1.00      1.00      1.00        14\n",
      "           8       0.92      1.00      0.96        23\n",
      "           9       1.00      1.00      1.00        20\n",
      "          10       1.00      1.00      1.00        11\n",
      "          11       1.00      1.00      1.00        21\n",
      "          12       1.00      1.00      1.00        19\n",
      "          13       1.00      1.00      1.00        24\n",
      "          14       1.00      1.00      1.00        19\n",
      "          15       1.00      1.00      1.00        17\n",
      "          16       1.00      1.00      1.00        14\n",
      "          17       1.00      1.00      1.00        23\n",
      "          18       1.00      1.00      1.00        23\n",
      "          19       1.00      1.00      1.00        23\n",
      "          20       1.00      0.89      0.94        19\n",
      "          21       1.00      1.00      1.00        19\n",
      "\n",
      "    accuracy                           1.00       440\n",
      "   macro avg       1.00      1.00      1.00       440\n",
      "weighted avg       1.00      1.00      1.00       440\n",
      "\n",
      "Naive Bayes Accuracy: 0.9954545454545455\n"
     ]
    }
   ],
   "source": [
    "# 7. Train and Evaluate Naive Bayes Model\n",
    "nb_model = CustomNaiveBayes()\n",
    "nb_model.fit(xtrain, ytrain)\n",
    "nb_predictions = nb_model.predict(xtest)\n",
    "print(\"Naive Bayes Classification Report:\")\n",
    "print(classification_report(ytest, nb_predictions))\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_score(ytest, nb_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "42a98368-eb2f-4300-8ef4-3f2cadd66051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class for the given data: ['coffee']\n"
     ]
    }
   ],
   "source": [
    "# Testing using random data\n",
    "data = np.array([[104, 18, 30, 23.603016, 60.3, 6.7, 140.91]])\n",
    "# Make predictions using the Naive Bayes model\n",
    "prediction = nb_model.predict(data)\n",
    "predicted_class = label_encoder.inverse_transform([prediction])  # Convert numeric prediction back to original class\n",
    "print(\"Predicted Class for the given data:\", predicted_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
